{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NLP for English\n",
    "간단한 Tokenizing, Pos-tagging, Lemmatization을 해보고 실제 예제로 arXiv에서 scraping한 text mining 관련 논문의 초록들로 DocumentTermMatrix를 만들어봅니다. 이 때, DocumentTermMatrix의 Term으로는 초록에 등장한 명사(NN), 고유명사(NNP) 중 많이 출현한 100개의 단어로 Term을 만들고 그 것을 토대로 DocumentTermMatrix를 만들어봅니다.  \n",
    "  \n",
    "* _nltk와 sklearn의 sub-module인 feature-extraction.text를 활용합니다._  \n",
    "* 실제로 활용하셔도 좋으나 조금 더 코드를 다듬어서 사용하시는 것을 추천드립니다.\n",
    "* nltk : http://www.nltk.org/book/\n",
    "\n",
    "만든이 : 김보섭"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pos tagging for English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "#nltk.download() 필요한 것만 찾아서 다운받아도됩니다. (함수나 모듈)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And now for something completely different ['And', 'now', 'for', 'something', 'completely', 'different']\n"
     ]
    }
   ],
   "source": [
    "s1 = 'And now for something completely different'\n",
    "s1_tokens = nltk.word_tokenize(s1)\n",
    "print(s1,s1_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andzqqnowzqqforzqqsomethingzqqcompletelyzqqdifferent\n"
     ]
    }
   ],
   "source": [
    "s2 = 'zqq'.join(['And', 'now', 'for', 'something', 'completely', 'different'])\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andzqqnowzqqforzqqsomethingzqqcompletelyzqqdifferent ['And', 'now', 'for', 'something', 'completely', 'different']\n"
     ]
    }
   ],
   "source": [
    "s2_tokens = nltk.regexp_tokenize(text = s2, pattern = 'zqq', gaps = True)\n",
    "print(s2, s2_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "s1_tags= nltk.pos_tag(s1_tokens)\n",
    "print(s1_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('something', 'NN')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 명사만 추출\n",
    "[s1_tag for s1_tag in s1_tags if s1_tag[1] == 'NN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'illuminate'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize('illuminate', pos = 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### example\n",
    "arXiv에서 scraping한 text mining paper들의 abstract에서 DocumentTermMatrix의 Term을 명사로 한정한 DocumentTermMatrix 생성 (Term-frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### package load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'Scrapping text mining papers in arXiv.py',\n",
       " 'Simple NLP for English.ipynb',\n",
       " 'text_mining_paper.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os, sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>author</th>\n",
       "      <th>meta</th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The complicated, evolving landscape of cancer ...</td>\n",
       "      <td>Rocco Piazza, Daniele Ramazzotti, Roberta Spin...</td>\n",
       "      <td>Thu, 9 Mar 2017 01:24:23 GMT (948kb)</td>\n",
       "      <td>Genomics (q-bio.GN)</td>\n",
       "      <td>OncoScore: a novel, Internet-based tool to ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mining textual patterns in news, tweets, paper...</td>\n",
       "      <td>Meng Jiang, Jingbo Shang, Taylor Cassidy, Xian...</td>\n",
       "      <td>Mon, 13 Mar 2017 01:06:19 GMT (1150kb,D) [v2] ...</td>\n",
       "      <td>Computation and Language (cs.CL)</td>\n",
       "      <td>MetaPAD: Meta Pattern Discovery from Massive T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This paper is a tutorial on Formal Concept Ana...</td>\n",
       "      <td>Dmitry I. Ignatov</td>\n",
       "      <td>Wed, 8 Mar 2017 12:53:21 GMT (3541kb,D)</td>\n",
       "      <td>Information Retrieval (cs.IR)</td>\n",
       "      <td>Introduction to Formal Concept Analysis and It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Topic models have been widely used in discover...</td>\n",
       "      <td>Jarvan Law, Hankz Hankui Zhuo, Junhua He, Erhu...</td>\n",
       "      <td>Thu, 23 Feb 2017 07:16:03 GMT (96kb,D)</td>\n",
       "      <td>Computation and Language (cs.CL)</td>\n",
       "      <td>LTSG: Latent Topical Skip-Gram for Mutually Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Entity extraction is fundamental to many text ...</td>\n",
       "      <td>Zeyi Wen, Dong Deng, Rui Zhang, Kotagiri Ramam...</td>\n",
       "      <td>Sun, 12 Feb 2017 12:46:40 GMT (89kb)</td>\n",
       "      <td>Databases (cs.DB)</td>\n",
       "      <td>A Technical Report: Entity Extraction using Bo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  The complicated, evolving landscape of cancer ...   \n",
       "1  Mining textual patterns in news, tweets, paper...   \n",
       "2  This paper is a tutorial on Formal Concept Ana...   \n",
       "3  Topic models have been widely used in discover...   \n",
       "4  Entity extraction is fundamental to many text ...   \n",
       "\n",
       "                                              author  \\\n",
       "0  Rocco Piazza, Daniele Ramazzotti, Roberta Spin...   \n",
       "1  Meng Jiang, Jingbo Shang, Taylor Cassidy, Xian...   \n",
       "2                                  Dmitry I. Ignatov   \n",
       "3  Jarvan Law, Hankz Hankui Zhuo, Junhua He, Erhu...   \n",
       "4  Zeyi Wen, Dong Deng, Rui Zhang, Kotagiri Ramam...   \n",
       "\n",
       "                                                meta  \\\n",
       "0               Thu, 9 Mar 2017 01:24:23 GMT (948kb)   \n",
       "1  Mon, 13 Mar 2017 01:06:19 GMT (1150kb,D) [v2] ...   \n",
       "2            Wed, 8 Mar 2017 12:53:21 GMT (3541kb,D)   \n",
       "3             Thu, 23 Feb 2017 07:16:03 GMT (96kb,D)   \n",
       "4               Sun, 12 Feb 2017 12:46:40 GMT (89kb)   \n",
       "\n",
       "                            subject  \\\n",
       "0               Genomics (q-bio.GN)   \n",
       "1  Computation and Language (cs.CL)   \n",
       "2     Information Retrieval (cs.IR)   \n",
       "3  Computation and Language (cs.CL)   \n",
       "4                 Databases (cs.DB)   \n",
       "\n",
       "                                               title  \n",
       "0  OncoScore: a novel, Internet-based tool to ass...  \n",
       "1  MetaPAD: Meta Pattern Discovery from Massive T...  \n",
       "2  Introduction to Formal Concept Analysis and It...  \n",
       "3  LTSG: Latent Topical Skip-Gram for Mutually Le...  \n",
       "4  A Technical Report: Entity Extraction using Bo...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers = pd.read_csv('./text_mining_paper.csv', encoding = 'cp949')\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The complicated, evolving landscape of cancer mutations poses a formidable\\r\\nchallenge to identify cancer genes among the large lists of mutations typically\\r\\ngenerated in NGS experiments. The ability to prioritize these variants is\\r\\ntherefore of paramount importance. To address this issue we developed\\r\\nOncoScore, a text-mining tool that ranks genes according to their association\\r\\nwith cancer, based on available biomedical literature. Receiver operating\\r\\ncharacteristic curve and the area under the curve (AUC) metrics on manually\\r\\ncurated datasets confirmed the excellent discriminating capability of OncoScore\\r\\n(OncoScore cut-off threshold = 21.09; AUC = 90.3%, 95% CI: 88.1-92.5%),\\r\\nindicating that OncoScore provides useful results in cases where an efficient\\r\\nprioritization of cancer-associated genes is needed.\\r\\n',\n",
       " 'Mining textual patterns in news, tweets, papers, and many other kinds of text\\r\\ncorpora has been an active theme in text mining and NLP research. Previous\\r\\nstudies adopt a dependency parsing-based pattern discovery approach. However,\\r\\nthe parsing results lose rich context around entities in the patterns, and the\\r\\nprocess is costly for a corpus of large scale. In this study, we propose a\\r\\nnovel typed textual pattern structure, called meta pattern, which is extended\\r\\nto a frequent, informative, and precise subsequence pattern in certain context.\\r\\nWe propose an efficient framework, called MetaPAD, which discovers meta\\r\\npatterns from massive corpora with three techniques: (1) it develops a\\r\\ncontext-aware segmentation method to carefully determine the boundaries of\\r\\npatterns with a learnt pattern quality assessment function, which avoids costly\\r\\ndependency parsing and generates high-quality patterns; (2) it identifies and\\r\\ngroups synonymous meta patterns from multiple facets---their types, contexts,\\r\\nand extractions; and (3) it examines type distributions of entities in the\\r\\ninstances extracted by each group of patterns, and looks for appropriate type\\r\\nlevels to make discovered patterns precise. Experiments demonstrate that our\\r\\nproposed framework discovers high-quality typed textual patterns efficiently\\r\\nfrom different genres of massive corpora and facilitates information\\r\\nextraction.\\r\\n',\n",
       " 'This paper is a tutorial on Formal Concept Analysis (FCA) and its\\r\\napplications. FCA is an applied branch of Lattice Theory, a mathematical\\r\\ndiscipline which enables formalisation of concepts as basic units of human\\r\\nthinking and analysing data in the object-attribute form. Originated in early\\r\\n80s, during the last three decades, it became a popular human-centred tool for\\r\\nknowledge representation and data analysis with numerous applications. Since\\r\\nthe tutorial was specially prepared for RuSSIR 2014, the covered FCA topics\\r\\ninclude Information Retrieval with a focus on visualisation aspects, Machine\\r\\nLearning, Data Mining and Knowledge Discovery, Text Mining and several others.\\r\\n',\n",
       " 'Topic models have been widely used in discovering latent topics which are\\r\\nshared across documents in text mining. Vector representations, word embeddings\\r\\nand topic embeddings, map words and topics into a low-dimensional and dense\\r\\nreal-value vector space, which have obtained high performance in NLP tasks.\\r\\nHowever, most of the existing models assume the result trained by one of them\\r\\nare perfect correct and used as prior knowledge for improving the other model.\\r\\nSome other models use the information trained from external large corpus to\\r\\nhelp improving smaller corpus. In this paper, we aim to build such an algorithm\\r\\nframework that makes topic models and vector representations mutually improve\\r\\neach other within the same corpus. An EM-style algorithm framework is employed\\r\\nto iteratively optimize both topic model and vector representations.\\r\\nExperimental results show that our model outperforms state-of-art methods on\\r\\nvarious NLP tasks.\\r\\n',\n",
       " 'Entity extraction is fundamental to many text mining tasks such as\\r\\norganisation name recognition. A popular approach to entity extraction is based\\r\\non matching sub-string candidates in a document against a dictionary of\\r\\nentities. To handle spelling errors and name variations of entities, usually\\r\\nthe matching is approximate and edit or Jaccard distance is used to measure\\r\\ndissimilarity between sub-string candidates and the entities. For approximate\\r\\nentity extraction from free text, existing work considers solely\\r\\ncharacter-based or solely token-based similarity and hence cannot\\r\\nsimultaneously deal with minor variations at token level and typos. In this\\r\\npaper, we address this problem by considering both character-based similarity\\r\\nand token-based similarity (i.e. two-level similarity). Measuring one-level\\r\\n(e.g. character-based) similarity is computationally expensive, and measuring\\r\\ntwo-level similarity is dramatically more expensive. By exploiting the\\r\\nproperties of the two-level similarity and the weights of tokens, we develop\\r\\nnovel techniques to significantly reduce the number of sub-string candidates\\r\\nthat require computation of two-level similarity against the dictionary of\\r\\nentities. A comprehensive experimental study on real world datasets show that\\r\\nour algorithm can efficiently extract entities from documents and produce a\\r\\nhigh F1 score in the range of [0.91, 0.97].\\r\\n',\n",
       " 'Explicit concept space models have proven efficacy for text representation in\\r\\nmany natural language and text mining applications. The idea is to embed\\r\\ntextual structures into a semantic space of concepts which captures the main\\r\\ntopics of these structures. That so called bag-of-concepts representation\\r\\nsuffers from data sparsity causing low similarity scores between similar texts\\r\\ndue to low concept overlap. In this paper we propose two neural embedding\\r\\nmodels in order to learn continuous concept vectors. Once learned, we propose\\r\\nan efficient vector aggregation method to generate fully dense bag-of-concepts\\r\\nrepresentations. Empirical results on a benchmark dataset for measuring entity\\r\\nsemantic relatedness show superior performance over other concept embedding\\r\\nmodels. In addition, by utilizing our efficient aggregation method, we\\r\\ndemonstrate the effectiveness of the densified vector representation over the\\r\\ntypical sparse representations for dataless classification where we can achieve\\r\\nat least same or better accuracy with much less dimensions.\\r\\n']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmps = list(papers['abstract'])\n",
    "tmps[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3글자 이상의 영어단어만 가져와서 tokenizing 하기\n",
    "# 3글자 이상의 영어단어에 pos-tagging\n",
    "tokenized = [re.findall('[A-z]{3,}', tmp) for tmp in tmps]\n",
    "tmps = list(map(lambda x : nltk.pos_tag(x), tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3글자 이상의 단어중 명사(NN), 고유명사(NNP)만 가져와서 Corpus 생성\n",
    "tmps = [list(filter(lambda x : x[1] == 'NN' or x[1] == 'NNP', tmp)) for tmp in tmps]\n",
    "tmps = [list(map(lambda x: x[0],tmp)) for tmp in tmps]\n",
    "tmps = [' '.join(tmp) for tmp in tmps]\n",
    "len(tmps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term을 설정 (tf기준 상위 100개 단어 추출)\n",
    "from collections import Counter\n",
    "word_count = list(map(lambda x : nltk.word_tokenize(x), tmps))\n",
    "word_count = sum(word_count, [])\n",
    "word_count = Counter(word_count)\n",
    "Vocabulary = word_count.most_common(100)\n",
    "Vocabulary = [tmp[0] for tmp in Vocabulary]\n",
    "len(Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DocumentTermMatrix 생성\n",
    "dtm = CountVectorizer(vocabulary=Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.fit(tmps)\n",
    "dtm.vocabulary_ # DocumentTermMatrix의 Term\n",
    "len(dtm.vocabulary_) # Term의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_dtm= dtm.transform(tmps).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dtm.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
