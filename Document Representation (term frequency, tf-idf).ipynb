{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Representation\n",
    "문서를 DocumentTermMatrix로 표현하는 방법을 다루며 Term weighting 방식으로 term frequency, tf-idf를 활용해봅니다. 실제 예제로 arXiv에서 scraping한 text mining 관련 논문의 초록들로 DocumentTermMatrix (tf, tf-idf)를 만들어봅니다. 이 떄 Term은 명사계열(NN?)만 활용하고 많이 출현한 100개 단어만 활용합니다. 또한 DocumentTermMatrix를 만들 때, tokenizer를 정의해서 활용합니다.\n",
    "  \n",
    "* _nltk와 sklearn의 sub-module인 feature-extraction.text를 활용합니다._\n",
    "* nltk : http://www.nltk.org/book/\n",
    "* gensim : https://radimrehurek.com/gensim/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DocumentTermMatrix with tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = ['I want to enjoy things and have fun and live like every day is the last day',\n",
    "         'When the Lore closes a door, somewher he opens a window',\n",
    "         'Great power always comes with Great responsibility',\n",
    "         'Luck favors the prepared']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3글자 이상의 영어단어만 각 문서에서 추출\n",
    "corpus = ['//'.join(re.findall('[A-z]{3,}', doc)) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def string_tokenizer(doc):\n",
    "    return doc.split('//')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenizer option에 내가 정의한 tokenizer를 명시\n",
    "# min_df로 최소 토큰의 빈도 설정 (Term의 list를 먼저 만들어두지 않았을 경우 활용할만한 옵션)\n",
    "tf_dtm = CountVectorizer(tokenizer = string_tokenizer, min_df = 1).fit(corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 0, 0, 2, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "        1, 0, 0, 1, 1, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_dtm.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DocumentTermMatrix with tf-idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_dtm= TfidfVectorizer(tokenizer = string_tokenizer).fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.47936124,  0.        ,  0.        ,  0.47936124,\n",
       "         0.        ,  0.23968062,  0.23968062,  0.        ,  0.23968062,\n",
       "         0.        ,  0.23968062,  0.23968062,  0.23968062,  0.23968062,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.15298503,  0.23968062,  0.23968062,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.36742339,  0.        ,  0.        ,\n",
       "         0.36742339,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.36742339,  0.        ,  0.36742339,  0.        ,  0.        ,\n",
       "         0.        ,  0.36742339,  0.23452159,  0.        ,  0.        ,\n",
       "         0.36742339,  0.36742339,  0.        ],\n",
       "       [ 0.33333333,  0.        ,  0.        ,  0.33333333,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.66666667,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.33333333,  0.        ,\n",
       "         0.33333333,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.33333333],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.5417361 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.5417361 ,  0.        ,  0.        ,  0.5417361 ,\n",
       "         0.        ,  0.        ,  0.34578314,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_dtm.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'Document Representation (term frequency, tf-idf).ipynb',\n",
       " 'Scrapping text mining papers in arXiv.py',\n",
       " 'Simple NLP for English.ipynb',\n",
       " 'Simple NLP for Korean.ipynb',\n",
       " 'text_mining_paper.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>author</th>\n",
       "      <th>meta</th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The complicated, evolving landscape of cancer ...</td>\n",
       "      <td>Rocco Piazza, Daniele Ramazzotti, Roberta Spin...</td>\n",
       "      <td>Thu, 9 Mar 2017 01:24:23 GMT (948kb)</td>\n",
       "      <td>Genomics (q-bio.GN)</td>\n",
       "      <td>OncoScore: a novel, Internet-based tool to ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mining textual patterns in news, tweets, paper...</td>\n",
       "      <td>Meng Jiang, Jingbo Shang, Taylor Cassidy, Xian...</td>\n",
       "      <td>Mon, 13 Mar 2017 01:06:19 GMT (1150kb,D) [v2] ...</td>\n",
       "      <td>Computation and Language (cs.CL)</td>\n",
       "      <td>MetaPAD: Meta Pattern Discovery from Massive T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This paper is a tutorial on Formal Concept Ana...</td>\n",
       "      <td>Dmitry I. Ignatov</td>\n",
       "      <td>Wed, 8 Mar 2017 12:53:21 GMT (3541kb,D)</td>\n",
       "      <td>Information Retrieval (cs.IR)</td>\n",
       "      <td>Introduction to Formal Concept Analysis and It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Topic models have been widely used in discover...</td>\n",
       "      <td>Jarvan Law, Hankz Hankui Zhuo, Junhua He, Erhu...</td>\n",
       "      <td>Thu, 23 Feb 2017 07:16:03 GMT (96kb,D)</td>\n",
       "      <td>Computation and Language (cs.CL)</td>\n",
       "      <td>LTSG: Latent Topical Skip-Gram for Mutually Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Entity extraction is fundamental to many text ...</td>\n",
       "      <td>Zeyi Wen, Dong Deng, Rui Zhang, Kotagiri Ramam...</td>\n",
       "      <td>Sun, 12 Feb 2017 12:46:40 GMT (89kb)</td>\n",
       "      <td>Databases (cs.DB)</td>\n",
       "      <td>A Technical Report: Entity Extraction using Bo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  The complicated, evolving landscape of cancer ...   \n",
       "1  Mining textual patterns in news, tweets, paper...   \n",
       "2  This paper is a tutorial on Formal Concept Ana...   \n",
       "3  Topic models have been widely used in discover...   \n",
       "4  Entity extraction is fundamental to many text ...   \n",
       "\n",
       "                                              author  \\\n",
       "0  Rocco Piazza, Daniele Ramazzotti, Roberta Spin...   \n",
       "1  Meng Jiang, Jingbo Shang, Taylor Cassidy, Xian...   \n",
       "2                                  Dmitry I. Ignatov   \n",
       "3  Jarvan Law, Hankz Hankui Zhuo, Junhua He, Erhu...   \n",
       "4  Zeyi Wen, Dong Deng, Rui Zhang, Kotagiri Ramam...   \n",
       "\n",
       "                                                meta  \\\n",
       "0               Thu, 9 Mar 2017 01:24:23 GMT (948kb)   \n",
       "1  Mon, 13 Mar 2017 01:06:19 GMT (1150kb,D) [v2] ...   \n",
       "2            Wed, 8 Mar 2017 12:53:21 GMT (3541kb,D)   \n",
       "3             Thu, 23 Feb 2017 07:16:03 GMT (96kb,D)   \n",
       "4               Sun, 12 Feb 2017 12:46:40 GMT (89kb)   \n",
       "\n",
       "                            subject  \\\n",
       "0               Genomics (q-bio.GN)   \n",
       "1  Computation and Language (cs.CL)   \n",
       "2     Information Retrieval (cs.IR)   \n",
       "3  Computation and Language (cs.CL)   \n",
       "4                 Databases (cs.DB)   \n",
       "\n",
       "                                               title  \n",
       "0  OncoScore: a novel, Internet-based tool to ass...  \n",
       "1  MetaPAD: Meta Pattern Discovery from Massive T...  \n",
       "2  Introduction to Formal Concept Analysis and It...  \n",
       "3  LTSG: Latent Topical Skip-Gram for Mutually Le...  \n",
       "4  A Technical Report: Entity Extraction using Bo...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers = pd.read_csv('./text_mining_paper.csv', encoding = 'cp949')\n",
    "print(papers.shape)\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The complicated, evolving landscape of cancer mutations poses a formidable\\r\\nchallenge to identify cancer genes among the large lists of mutations typically\\r\\ngenerated in NGS experiments. The ability to prioritize these variants is\\r\\ntherefore of paramount importance. To address this issue we developed\\r\\nOncoScore, a text-mining tool that ranks genes according to their association\\r\\nwith cancer, based on available biomedical literature. Receiver operating\\r\\ncharacteristic curve and the area under the curve (AUC) metrics on manually\\r\\ncurated datasets confirmed the excellent discriminating capability of OncoScore\\r\\n(OncoScore cut-off threshold = 21.09; AUC = 90.3%, 95% CI: 88.1-92.5%),\\r\\nindicating that OncoScore provides useful results in cases where an efficient\\r\\nprioritization of cancer-associated genes is needed.\\r\\n',\n",
       " 'Mining textual patterns in news, tweets, papers, and many other kinds of text\\r\\ncorpora has been an active theme in text mining and NLP research. Previous\\r\\nstudies adopt a dependency parsing-based pattern discovery approach. However,\\r\\nthe parsing results lose rich context around entities in the patterns, and the\\r\\nprocess is costly for a corpus of large scale. In this study, we propose a\\r\\nnovel typed textual pattern structure, called meta pattern, which is extended\\r\\nto a frequent, informative, and precise subsequence pattern in certain context.\\r\\nWe propose an efficient framework, called MetaPAD, which discovers meta\\r\\npatterns from massive corpora with three techniques: (1) it develops a\\r\\ncontext-aware segmentation method to carefully determine the boundaries of\\r\\npatterns with a learnt pattern quality assessment function, which avoids costly\\r\\ndependency parsing and generates high-quality patterns; (2) it identifies and\\r\\ngroups synonymous meta patterns from multiple facets---their types, contexts,\\r\\nand extractions; and (3) it examines type distributions of entities in the\\r\\ninstances extracted by each group of patterns, and looks for appropriate type\\r\\nlevels to make discovered patterns precise. Experiments demonstrate that our\\r\\nproposed framework discovers high-quality typed textual patterns efficiently\\r\\nfrom different genres of massive corpora and facilitates information\\r\\nextraction.\\r\\n',\n",
       " 'This paper is a tutorial on Formal Concept Analysis (FCA) and its\\r\\napplications. FCA is an applied branch of Lattice Theory, a mathematical\\r\\ndiscipline which enables formalisation of concepts as basic units of human\\r\\nthinking and analysing data in the object-attribute form. Originated in early\\r\\n80s, during the last three decades, it became a popular human-centred tool for\\r\\nknowledge representation and data analysis with numerous applications. Since\\r\\nthe tutorial was specially prepared for RuSSIR 2014, the covered FCA topics\\r\\ninclude Information Retrieval with a focus on visualisation aspects, Machine\\r\\nLearning, Data Mining and Knowledge Discovery, Text Mining and several others.\\r\\n']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = list(papers['abstract'])\n",
    "corpus[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Custom tokenizer\n",
    "# 3글자 이상의 영어단어중 명사계열만 tokenize하는 tokenizer\n",
    "def my_tokenizer(doc):\n",
    "    tmp = re.findall('[A-z]{3,}', doc)\n",
    "    tmp = [token[0] for token in nltk.pos_tag(tmp) if token[1].find('NN') == 0]\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term의 목록을 미리 설정\n",
    "from collections import Counter\n",
    "tmps = [re.findall('[A-z]{3,}', doc) for doc in corpus]\n",
    "tmps = sum(tmps, [])\n",
    "tmps = [token[0] for token in nltk.pos_tag(tmps) if token[1].find('NN') == 0]\n",
    "word_count = Counter(tmps)\n",
    "voca = [token[0] for token in word_count.most_common(100)]\n",
    "len(voca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DocumentTermMatrix with tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_dtm = CountVectorizer(tokenizer = my_tokenizer, vocabulary = voca).fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf_dtm.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 2, 3, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [1, 3, 2, ..., 0, 0, 0],\n",
       "       [1, 3, 0, ..., 0, 0, 0],\n",
       "       [4, 2, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tf_dtm= tf_dtm.transform(corpus).toarray()\n",
    "my_tf_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tf_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DocumentTermMatrix with tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_dtm = TfidfVectorizer(tokenizer = my_tokenizer, vocabulary = voca).fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_dtm.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.18524405,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.04411306,  0.0397914 ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.10389214,  0.18742813,  0.39033973, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.07425438,  0.20093954,  0.18599055, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.08228962,  0.22268367,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.55188593,  0.24890946,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tfidf_dtm = tfidf_dtm.transform(corpus).toarray()\n",
    "my_tfidf_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 100)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tf_dtm.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
