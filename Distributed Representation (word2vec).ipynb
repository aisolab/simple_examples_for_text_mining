{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Representation - word2vec\n",
    "단어를 Continuous vector로 표현하는 방법인 word2vec (skip-gram)을 해봅니다. 실제 예제로 arXiv에서 scraping한 text mining 관련 논문의 초록의 단어들로 진행합니다.\n",
    "  \n",
    "* _nltk를 활용합니다._\n",
    "* gensim을 활용합니다.\n",
    "* nltk : http://www.nltk.org/book/\n",
    "* gensim : https://radimrehurek.com/gensim/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec\n",
    "### Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Slow version of gensim.models.doc2vec is being used\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'Distributed Representation (word2vec).ipynb',\n",
       " 'Document Representation (term frequency, tf-idf).ipynb',\n",
       " 'Scrapping text mining papers in arXiv.py',\n",
       " 'Simple NLP for English.ipynb',\n",
       " 'Simple NLP for Korean.ipynb',\n",
       " 'text_mining_paper.csv']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load abstracts of text mining papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>author</th>\n",
       "      <th>meta</th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The complicated, evolving landscape of cancer ...</td>\n",
       "      <td>Rocco Piazza, Daniele Ramazzotti, Roberta Spin...</td>\n",
       "      <td>Thu, 9 Mar 2017 01:24:23 GMT (948kb)</td>\n",
       "      <td>Genomics (q-bio.GN)</td>\n",
       "      <td>OncoScore: a novel, Internet-based tool to ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mining textual patterns in news, tweets, paper...</td>\n",
       "      <td>Meng Jiang, Jingbo Shang, Taylor Cassidy, Xian...</td>\n",
       "      <td>Mon, 13 Mar 2017 01:06:19 GMT (1150kb,D) [v2] ...</td>\n",
       "      <td>Computation and Language (cs.CL)</td>\n",
       "      <td>MetaPAD: Meta Pattern Discovery from Massive T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This paper is a tutorial on Formal Concept Ana...</td>\n",
       "      <td>Dmitry I. Ignatov</td>\n",
       "      <td>Wed, 8 Mar 2017 12:53:21 GMT (3541kb,D)</td>\n",
       "      <td>Information Retrieval (cs.IR)</td>\n",
       "      <td>Introduction to Formal Concept Analysis and It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Topic models have been widely used in discover...</td>\n",
       "      <td>Jarvan Law, Hankz Hankui Zhuo, Junhua He, Erhu...</td>\n",
       "      <td>Thu, 23 Feb 2017 07:16:03 GMT (96kb,D)</td>\n",
       "      <td>Computation and Language (cs.CL)</td>\n",
       "      <td>LTSG: Latent Topical Skip-Gram for Mutually Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Entity extraction is fundamental to many text ...</td>\n",
       "      <td>Zeyi Wen, Dong Deng, Rui Zhang, Kotagiri Ramam...</td>\n",
       "      <td>Sun, 12 Feb 2017 12:46:40 GMT (89kb)</td>\n",
       "      <td>Databases (cs.DB)</td>\n",
       "      <td>A Technical Report: Entity Extraction using Bo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  The complicated, evolving landscape of cancer ...   \n",
       "1  Mining textual patterns in news, tweets, paper...   \n",
       "2  This paper is a tutorial on Formal Concept Ana...   \n",
       "3  Topic models have been widely used in discover...   \n",
       "4  Entity extraction is fundamental to many text ...   \n",
       "\n",
       "                                              author  \\\n",
       "0  Rocco Piazza, Daniele Ramazzotti, Roberta Spin...   \n",
       "1  Meng Jiang, Jingbo Shang, Taylor Cassidy, Xian...   \n",
       "2                                  Dmitry I. Ignatov   \n",
       "3  Jarvan Law, Hankz Hankui Zhuo, Junhua He, Erhu...   \n",
       "4  Zeyi Wen, Dong Deng, Rui Zhang, Kotagiri Ramam...   \n",
       "\n",
       "                                                meta  \\\n",
       "0               Thu, 9 Mar 2017 01:24:23 GMT (948kb)   \n",
       "1  Mon, 13 Mar 2017 01:06:19 GMT (1150kb,D) [v2] ...   \n",
       "2            Wed, 8 Mar 2017 12:53:21 GMT (3541kb,D)   \n",
       "3             Thu, 23 Feb 2017 07:16:03 GMT (96kb,D)   \n",
       "4               Sun, 12 Feb 2017 12:46:40 GMT (89kb)   \n",
       "\n",
       "                            subject  \\\n",
       "0               Genomics (q-bio.GN)   \n",
       "1  Computation and Language (cs.CL)   \n",
       "2     Information Retrieval (cs.IR)   \n",
       "3  Computation and Language (cs.CL)   \n",
       "4                 Databases (cs.DB)   \n",
       "\n",
       "                                               title  \n",
       "0  OncoScore: a novel, Internet-based tool to ass...  \n",
       "1  MetaPAD: Meta Pattern Discovery from Massive T...  \n",
       "2  Introduction to Formal Concept Analysis and It...  \n",
       "3  LTSG: Latent Topical Skip-Gram for Mutually Le...  \n",
       "4  A Technical Report: Entity Extraction using Bo...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers = pd.read_csv('./text_mining_paper.csv', encoding = 'cp949')\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abstracts = list(papers['abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "1. 2글자 이상의 영단어 추출, 모두 소문자로 변환\n",
    "2. gensim의 Word2Vec class가 input으로 받을 수 있는 corpus 형태로 변환  \n",
    "(nested list의 형태이며 큰 요소의 list가 되는 list는 순서가 바뀌지않은 token의 집합)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = list(map(lambda x : re.findall('[A-z]{2,}',x.lower()), abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'complicated', 'evolving', 'of', 'cancer']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 유사도 검증을 위해 token들의 집합을 하나 따로 떼어두기\n",
    "from collections import Counter\n",
    "tokens = sum(corpus, [])\n",
    "tokens = Counter(tokens)\n",
    "tokens = [token[0] for token in list(tokens.items()) if token[1] >= 2]\n",
    "tokens[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training word2vec\n",
    "관련하여 자세한 옵션은 공식 문서를 참조할 것, 본 예제에서는 다음과 같은 parameter로 training  \n",
    "참고 : https://radimrehurek.com/gensim/models/word2vec.html\n",
    "1. 100차원의 벡터로 embedding\n",
    "2. 초기 learning rate = 0.025\n",
    "3. window size = 5\n",
    "5. min_count = 2 (최소 2회이상 나타난 단어만)\n",
    "4. skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:772: UserWarning: C extension not loaded for Word2Vec, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\"C extension not loaded for Word2Vec, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "config = {'size' :100, 'alpha' : 0.025, 'window' : 5, 'sg' : 1, 'min_count' : 2} \n",
    "# sg는 skip-gram 방법의 사용여부요 1이면 skip-gram, 0이면 CBOW\n",
    "model = Word2Vec(sentences = corpus, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.init_sims(replace = True) #필요없는 메모리 unload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the complicated\n"
     ]
    }
   ],
   "source": [
    "# 두 word간의 embedding 공간상의 유사도 계산\n",
    "model.similarity(tokens[0], tokens[1])\n",
    "print(tokens[0], tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('each', 0.9989919066429138),\n",
       " ('number', 0.9989718794822693),\n",
       " ('problem', 0.9989573955535889),\n",
       " ('words', 0.9989050030708313),\n",
       " ('one', 0.9988996982574463),\n",
       " ('is', 0.9988754391670227),\n",
       " ('further', 0.9988439679145813),\n",
       " ('by', 0.9988322257995605),\n",
       " ('both', 0.9988305568695068),\n",
       " ('concepts', 0.9988234043121338),\n",
       " ('topics', 0.9988216757774353),\n",
       " ('content', 0.9988120198249817),\n",
       " ('these', 0.9987977147102356),\n",
       " ('system', 0.9987806081771851),\n",
       " ('web', 0.9987776279449463),\n",
       " ('them', 0.9987676739692688),\n",
       " ('process', 0.9987626075744629),\n",
       " ('not', 0.9987606406211853),\n",
       " ('features', 0.9987539649009705),\n",
       " ('result', 0.9987524747848511)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 유사한 20개 단어 출력\n",
    "model.most_similar(tokens[0], topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('because', 0.9987838268280029),\n",
       " ('type', 0.9987679719924927),\n",
       " ('context', 0.9987528324127197),\n",
       " ('operations', 0.9987417459487915),\n",
       " ('up', 0.9987417459487915),\n",
       " ('attributes', 0.9987099766731262),\n",
       " ('detection', 0.9987073540687561),\n",
       " ('embedding', 0.9987009167671204),\n",
       " ('traditional', 0.998698353767395),\n",
       " ('was', 0.998693585395813)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어간 관계 찾기\n",
    "model.most_similar(positive = ['experimental', 'words'], negative = ['that'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'of', 'and', 'to', 'in']\n",
      "2213\n"
     ]
    }
   ],
   "source": [
    "# word embedding이 산출된 word의 목록\n",
    "print(model.wv.index2word[0:5])\n",
    "print(len(model.wv.index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 모든 word들의 embedding matrix 생성\n",
    "import numpy as np\n",
    "my_word = model.wv.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.01988189,  0.13139629,  0.01627134, -0.0089903 ,  0.11221685,\n",
       "         0.17640017,  0.00949641,  0.21792442, -0.15304418,  0.13018884,\n",
       "         0.13977255, -0.16033855,  0.10094168, -0.10551531, -0.21017449,\n",
       "         0.17190437, -0.04920886,  0.06440064,  0.07505376, -0.06977551,\n",
       "         0.0194921 ,  0.02675535,  0.004789  , -0.03555432,  0.00410689,\n",
       "         0.12984014, -0.08025134,  0.03181918,  0.06659508, -0.03575774,\n",
       "        -0.00317442,  0.01312721, -0.10238264,  0.05749114,  0.10639635,\n",
       "         0.03436963,  0.02878631,  0.0412176 ,  0.10581778,  0.02268814,\n",
       "        -0.26732701, -0.18780009,  0.10348438,  0.11659094, -0.08257636,\n",
       "        -0.09209964,  0.03796738,  0.08150806,  0.14146197,  0.03125246,\n",
       "        -0.08332349,  0.0649891 ,  0.03386664, -0.08881352, -0.08040347,\n",
       "         0.09037085, -0.09085443,  0.0528989 ,  0.06728957,  0.00124398,\n",
       "         0.01503391,  0.04555376,  0.13048171, -0.06746729,  0.13963263,\n",
       "        -0.07229198,  0.01079031,  0.03317163, -0.14355727,  0.07231617,\n",
       "        -0.0806184 , -0.01704074,  0.11890336,  0.06467842,  0.09631699,\n",
       "        -0.01747986,  0.0742498 ,  0.07719731,  0.19152981, -0.05698772,\n",
       "         0.00695872,  0.14440885,  0.06512381, -0.05852557,  0.0500371 ,\n",
       "        -0.12078544, -0.16694328, -0.02543714, -0.11203332,  0.05100216,\n",
       "        -0.17476679,  0.14888267, -0.14754048,  0.0771755 , -0.06683412,\n",
       "        -0.0452286 , -0.13593417,  0.03766298,  0.1745799 ,  0.0748502 ], dtype=float32),\n",
       " array([ 0.02363584,  0.13388114,  0.01152299, -0.01572002,  0.10397811,\n",
       "         0.17312485,  0.01297095,  0.2235039 , -0.14899652,  0.12915355,\n",
       "         0.13502283, -0.16275197,  0.09929111, -0.10908275, -0.20572814,\n",
       "         0.17185545, -0.04882769,  0.06182609,  0.08006645, -0.06490146,\n",
       "         0.02270635,  0.02570301,  0.0116428 , -0.04118173, -0.00044146,\n",
       "         0.13773194, -0.07739158,  0.04104508,  0.07270178, -0.03438018,\n",
       "        -0.00235091,  0.00354476, -0.10372209,  0.06137   ,  0.10603714,\n",
       "         0.03301552,  0.01982182,  0.04517582,  0.11015701,  0.02533032,\n",
       "        -0.27036598, -0.18954277,  0.11205012,  0.12275258, -0.08576325,\n",
       "        -0.07877035,  0.04006467,  0.07873272,  0.13974009,  0.04230386,\n",
       "        -0.09320041,  0.06007348,  0.02178494, -0.08199757, -0.08023334,\n",
       "         0.08664757, -0.09214042,  0.06143573,  0.07186422,  0.00644944,\n",
       "         0.01758284,  0.0396597 ,  0.12604883, -0.06143548,  0.1405098 ,\n",
       "        -0.06980989,  0.00645064,  0.03336406, -0.156735  ,  0.06504358,\n",
       "        -0.08566777, -0.02391797,  0.12499002,  0.06466357,  0.09351624,\n",
       "        -0.02058341,  0.07669181,  0.08469239,  0.18501233, -0.05966247,\n",
       "         0.00711869,  0.14110145,  0.06850017, -0.05631465,  0.05498234,\n",
       "        -0.11501104, -0.161577  , -0.02215823, -0.10872687,  0.05763356,\n",
       "        -0.17387192,  0.14887792, -0.14985397,  0.06989957, -0.06036117,\n",
       "        -0.03717192, -0.13443708,  0.04309698,  0.16775359,  0.07246476], dtype=float32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = [model[token] for token in my_word]\n",
    "embedding[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2213, 100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = np.asarray(embedding)\n",
    "embedding.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
